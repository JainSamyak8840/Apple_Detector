{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Object_Detector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eE8ZXgJmm80",
        "colab_type": "code",
        "outputId": "bd926c9b-0a8c-42d3-fad9-447a36a8538c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPMMdPHwDmVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os \n",
        "os.chdir('/content/drive/My Drive/objectDetection')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xI8EIaP-DrB9",
        "colab_type": "code",
        "outputId": "def2a3a0-acfe-4f0f-8877-c56e77b0fd4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        }
      },
      "source": [
        "!apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
        "!pip install -qq Cython contextlib2 pillow lxml matplotlib pycocotools"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package python-bs4.\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144542 files and directories currently installed.)\n",
            "Preparing to unpack .../0-python-bs4_4.6.0-1_all.deb ...\n",
            "Unpacking python-bs4 (4.6.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../1-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-chardet.\n",
            "Preparing to unpack .../2-python-chardet_3.0.4-1_all.deb ...\n",
            "Unpacking python-chardet (3.0.4-1) ...\n",
            "Selecting previously unselected package python-six.\n",
            "Preparing to unpack .../3-python-six_1.11.0-2_all.deb ...\n",
            "Unpacking python-six (1.11.0-2) ...\n",
            "Selecting previously unselected package python-webencodings.\n",
            "Preparing to unpack .../4-python-webencodings_0.5-2_all.deb ...\n",
            "Unpacking python-webencodings (0.5-2) ...\n",
            "Selecting previously unselected package python-html5lib.\n",
            "Preparing to unpack .../5-python-html5lib_0.999999999-1_all.deb ...\n",
            "Unpacking python-html5lib (0.999999999-1) ...\n",
            "Selecting previously unselected package python-lxml:amd64.\n",
            "Preparing to unpack .../6-python-lxml_4.2.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../7-python-olefile_0.45.1-1_all.deb ...\n",
            "Unpacking python-olefile (0.45.1-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../8-python-pil_5.1.0-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up python-six (1.11.0-2) ...\n",
            "Setting up python-bs4 (4.6.0-1) ...\n",
            "Setting up python-lxml:amd64 (4.2.1-1ubuntu0.1) ...\n",
            "Setting up python-olefile (0.45.1-1) ...\n",
            "Setting up python-pil:amd64 (5.1.0-1ubuntu0.2) ...\n",
            "Setting up python-webencodings (0.5-2) ...\n",
            "Setting up python-chardet (3.0.4-1) ...\n",
            "Setting up python-html5lib (0.999999999-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-9ACVmHVv6K",
        "colab_type": "code",
        "outputId": "5b424c2e-2784-4108-f8e5-b506aafc53e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "source": [
        "pip install tensorflow==1.15.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 41kB/s \n",
            "\u001b[?25hCollecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 61.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.18.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.27.2)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 56.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.9.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (3.2.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (46.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=715963c98d34cecd155e56edf88a94c0e6402c746c06b5f99ec11dc26cd2ef3d\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDHi9i2CEy1A",
        "colab_type": "code",
        "outputId": "aca8af48-c5a5-4e45-e216-ffc29f8b385d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "\n",
        "import re\n",
        "import os\n",
        "import io\n",
        "import glob\n",
        "import shutil\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import cv2 \n",
        "\n",
        "from PIL import Image\n",
        "from collections import namedtuple, OrderedDict\n",
        "\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLrTGKM3E8Gv",
        "colab_type": "code",
        "outputId": "3d313134-0a08-4d8f-ffb9-6b1521bd8ebc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tf.__version__)\n",
        "os.chdir('/content/drive/My Drive/objectDetection')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhD927WQi5dm",
        "colab_type": "code",
        "outputId": "11e9419e-039c-4924-be91-9ca7619c7a0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def xml_to_csv(path):\n",
        "  classes_names = []\n",
        "  xml_list = []\n",
        "\n",
        "  for xml_file in glob.glob(path + '/*.xml'):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    for member in root.findall('object'):\n",
        "      classes_names.append(member[0].text)\n",
        "      value = (root.find('filename').text,\n",
        "               int(root.find('size')[0].text),\n",
        "               int(root.find('size')[1].text),\n",
        "               member[0].text,\n",
        "               int(member[4][0].text),\n",
        "               int(member[4][1].text),\n",
        "               int(member[4][2].text),\n",
        "               int(member[4][3].text))\n",
        "      xml_list.append(value)\n",
        "  column_name = ['filename', 'width', 'height', 'class', 'xmin', 'ymin', 'xmax', 'ymax']\n",
        "  xml_df = pd.DataFrame(xml_list, columns=column_name) \n",
        "  classes_names = list(set(classes_names))\n",
        "  classes_names.sort()\n",
        "  return xml_df, classes_names\n",
        "\n",
        "for label_path in ['train', 'test']:\n",
        "  image_path = os.path.join(os.getcwd(), label_path)\n",
        "  xml_df, classes = xml_to_csv(label_path)\n",
        "  xml_df.to_csv(f'{label_path}.csv', index=None)\n",
        "  print(f'Successfully converted {label_path} xml to csv.')\n",
        "\n",
        "label_map_path = os.path.join(\"label_map.pbtxt\")\n",
        "pbtxt_content = \"\"\n",
        "\n",
        "for i, class_name in enumerate(classes):\n",
        "    pbtxt_content = (\n",
        "        pbtxt_content\n",
        "        + \"item {{\\n    id: {0}\\n    name: '{1}'\\n}}\\n\\n\".format(i + 1, class_name)\n",
        "    )\n",
        "pbtxt_content = pbtxt_content.strip()\n",
        "with open(label_map_path, \"w\") as f:\n",
        "    f.write(pbtxt_content)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully converted train xml to csv.\n",
            "Successfully converted test xml to csv.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amXKRfg3IMI_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# downloads the models\n",
        "!git clone --q https://github.com/tensorflow/models.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCI-ijfLIvOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/objectDetection/models/research')\n",
        "#/content/drive/My Drive/objectDetection/object_detection\n",
        "# compils the proto buffers\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n",
        "# exports PYTHONPATH environment var with research and slim paths\n",
        "os.environ['PYTHONPATH'] += ':./:./slim/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmNBeFz_RjKZ",
        "colab_type": "code",
        "outputId": "64de0971-2c93-47f5-a92a-db7d9d7bb458",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        }
      },
      "source": [
        "from tensorflow.python.compiler.tensorrt import trt_convert as trt\n",
        "os.chdir('/content/drive/My Drive/objectDetection/models/research')\n",
        "# testing the model builder\n",
        "!python3 object_detection/builders/model_builder_test.py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "Running tests under Python 3.6.9: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTest.test_create_experimental_model\n",
            "[       OK ] ModelBuilderTest.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[       OK ] ModelBuilderTest.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[       OK ] ModelBuilderTest.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[       OK ] ModelBuilderTest.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[       OK ] ModelBuilderTest.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[       OK ] ModelBuilderTest.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[       OK ] ModelBuilderTest.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTest.test_session\n",
            "[  SKIPPED ] ModelBuilderTest.test_session\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[       OK ] ModelBuilderTest.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "[       OK ] ModelBuilderTest.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 17 tests in 0.177s\n",
            "\n",
            "OK (skipped=1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8yoq74hXNRh",
        "colab_type": "code",
        "outputId": "ca97b0dd-4940-4b6f-bb39-455df49abb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from object_detection.utils import dataset_util\n",
        "\n",
        "\n",
        "#change this to the base directory where your data/ is \n",
        "data_base_url = '/content/drive/My Drive/objectDetection/'\n",
        "\n",
        "#location of images\n",
        "test_dir = data_base_url +'images/test'\n",
        "train_dir = data_base_url +'images/train'\n",
        "\n",
        "def class_text_to_int(row_label):\n",
        "  if row_label == 'apple':\n",
        "    return 1\n",
        "  else:\n",
        "    None\n",
        "\n",
        "def split(df, group):\n",
        "  data = namedtuple('data', ['filename', 'object'])\n",
        "  gb = df.groupby(group)\n",
        "  return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
        "\n",
        "def create_tf_example(group, path):\n",
        "\twith tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
        "\t  encoded_jpg = fid.read()\n",
        "\tencoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "\timage = Image.open(encoded_jpg_io)\n",
        "\twidth, height = image.size\n",
        "\tfilename = group.filename.encode('utf8')\n",
        "\timage_format = b'jpg'\n",
        "\txmins = []\n",
        "\txmaxs = []\n",
        "\tymins = []\n",
        "\tymaxs = []\n",
        "\tclasses_text = []\n",
        "\tclasses = []\n",
        "\n",
        "\tfor index, row in group.object.iterrows():\n",
        "\t\txmins.append(row['xmin'] / width)\n",
        "\t\txmaxs.append(row['xmax'] / width)\n",
        "\t\tymins.append(row['ymin'] / height)\n",
        "\t\tymaxs.append(row['ymax'] / height)\n",
        "\t\tclasses_text.append(row['class'].encode('utf8'))\n",
        "\t\tclasses.append(class_text_to_int(row['class']))\n",
        "\n",
        "\ttf_example = tf.train.Example(features=tf.train.Features(feature={\n",
        "\t\t'image/height': dataset_util.int64_feature(height),\n",
        "\t\t'image/width': dataset_util.int64_feature(width),\n",
        "\t\t'image/filename': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/source_id': dataset_util.bytes_feature(filename),\n",
        "\t\t'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
        "\t\t'image/format': dataset_util.bytes_feature(image_format),\n",
        "\t\t'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
        "\t\t'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
        "\t\t'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
        "\t\t'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
        "\t\t'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
        "\t\t'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
        "\t\t}))\n",
        "\treturn tf_example\n",
        "#creates tfrecord for both csv's\n",
        "for csv in ['train', 'test']:\n",
        "  writer = tf.io.TFRecordWriter(data_base_url + csv + '.record')\n",
        "  if csv=='train':\n",
        "      path = os.path.join(train_dir)\n",
        "  else:\n",
        "      path=os.path.join(test_dir)\n",
        "  examples = pd.read_csv(data_base_url + csv + '.csv')\n",
        "  grouped = split(examples, 'filename')\n",
        "  for group in grouped:\n",
        "    tf_example = create_tf_example(group, path)\n",
        "    writer.write(tf_example.SerializeToString())\n",
        "    \n",
        "  writer.close()\n",
        "  output_path = os.path.join(os.getcwd(), data_base_url + csv + '.record')\n",
        "  print('Successfully created the TFRecords: {}'.format(data_base_url +csv + '.record'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully created the TFRecords: /content/drive/My Drive/objectDetection/train.record\n",
            "Successfully created the TFRecords: /content/drive/My Drive/objectDetection/test.record\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzN8KFUIklqs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some models to train on\n",
        "MODELS_CONFIG = {\n",
        "    'ssd_mobilenet_v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_coco_2018_03_29',\n",
        "    },\n",
        "    'faster_rcnn_inception_v2': {\n",
        "        'model_name': 'faster_rcnn_inception_v2_coco_2018_01_28',\n",
        "    },\n",
        "}\n",
        "\n",
        "# Select a model from `MODELS_CONFIG`.\n",
        "# I chose ssd_mobilenet_v2 for this project, you could choose any\n",
        "selected_model = 'ssd_mobilenet_v2'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEWmCw4uRdFw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#the distination folder where the model will be saved\n",
        "#change this if you have a different working dir\n",
        "DEST_DIR = '/content/drive/My Drive/objectDetection/models/research/pretrained_model'\n",
        "\n",
        "# Name of the object detection model to use.\n",
        "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
        "\n",
        "#selecting the model\n",
        "MODEL_FILE = MODEL + '.tar.gz'\n",
        "\n",
        "#creating the downlaod link for the model selected\n",
        "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
        "\n",
        "#checks if the model has already been downloaded, download it otherwise\n",
        "if not (os.path.exists(MODEL_FILE)):\n",
        "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
        "\n",
        "#unzipping the model and extracting its content\n",
        "tar = tarfile.open(MODEL_FILE)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# creating an output file to save the model while training\n",
        "os.remove(MODEL_FILE)\n",
        "if (os.path.exists(DEST_DIR)):\n",
        "    shutil.rmtree(DEST_DIR)\n",
        "os.rename(MODEL, DEST_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrmM_rORlwqg",
        "colab_type": "code",
        "outputId": "db25ba36-ffac-49fc-9c6b-ddb465b86087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "os.chdir('/content/drive/My Drive/objectDetection/models/research')\n",
        "#path to the config file\n",
        "!cat object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "# SSD with Mobilenet v2 configuration for MSCOCO Dataset.\n",
            "# Users should configure the fine_tune_checkpoint field in the train config as\n",
            "# well as the label_map_path and input_path fields in the train_input_reader and\n",
            "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
            "# should be configured.\n",
            "\n",
            "model {\n",
            "  ssd {\n",
            "    num_classes: 1\n",
            "    box_coder {\n",
            "      faster_rcnn_box_coder {\n",
            "        y_scale: 10.0\n",
            "        x_scale: 10.0\n",
            "        height_scale: 5.0\n",
            "        width_scale: 5.0\n",
            "      }\n",
            "    }\n",
            "    matcher {\n",
            "      argmax_matcher {\n",
            "        matched_threshold: 0.5\n",
            "        unmatched_threshold: 0.5\n",
            "        ignore_thresholds: false\n",
            "        negatives_lower_than_unmatched: true\n",
            "        force_match_for_each_row: true\n",
            "      }\n",
            "    }\n",
            "    similarity_calculator {\n",
            "      iou_similarity {\n",
            "      }\n",
            "    }\n",
            "    anchor_generator {\n",
            "      ssd_anchor_generator {\n",
            "        num_layers: 6\n",
            "        min_scale: 0.2\n",
            "        max_scale: 0.95\n",
            "        aspect_ratios: 1.0\n",
            "        aspect_ratios: 2.0\n",
            "        aspect_ratios: 0.5\n",
            "        aspect_ratios: 3.0\n",
            "        aspect_ratios: 0.3333\n",
            "      }\n",
            "    }\n",
            "    image_resizer {\n",
            "      fixed_shape_resizer {\n",
            "        height: 300\n",
            "        width: 300\n",
            "      }\n",
            "    }\n",
            "    box_predictor {\n",
            "      convolutional_box_predictor {\n",
            "        min_depth: 0\n",
            "        max_depth: 0\n",
            "        num_layers_before_predictor: 0\n",
            "        use_dropout: true\n",
            "        dropout_keep_probability: 0.8\n",
            "        kernel_size: 1\n",
            "        box_code_size: 4\n",
            "        apply_sigmoid_to_scores: false\n",
            "        conv_hyperparams {\n",
            "          activation: RELU_6,\n",
            "          regularizer {\n",
            "            l2_regularizer {\n",
            "              weight: 0.00004\n",
            "            }\n",
            "          }\n",
            "          initializer {\n",
            "            truncated_normal_initializer {\n",
            "              stddev: 0.03\n",
            "              mean: 0.0\n",
            "            }\n",
            "          }\n",
            "          batch_norm {\n",
            "            train: true,\n",
            "            scale: true,\n",
            "            center: true,\n",
            "            decay: 0.9997,\n",
            "            epsilon: 0.001,\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    feature_extractor {\n",
            "      type: 'ssd_mobilenet_v2'\n",
            "      min_depth: 16\n",
            "      depth_multiplier: 1.0\n",
            "      conv_hyperparams {\n",
            "        activation: RELU_6,\n",
            "        regularizer {\n",
            "          l2_regularizer {\n",
            "            weight: 0.00004\n",
            "          }\n",
            "        }\n",
            "        initializer {\n",
            "          truncated_normal_initializer {\n",
            "            stddev: 0.03\n",
            "            mean: 0.0\n",
            "          }\n",
            "        }\n",
            "        batch_norm {\n",
            "          train: true,\n",
            "          scale: true,\n",
            "          center: true,\n",
            "          decay: 0.9997,\n",
            "          epsilon: 0.001,\n",
            "        }\n",
            "      }\n",
            "    }\n",
            "    loss {\n",
            "      classification_loss {\n",
            "        weighted_sigmoid {\n",
            "        }\n",
            "      }\n",
            "      localization_loss {\n",
            "        weighted_smooth_l1 {\n",
            "        }\n",
            "      }\n",
            "      hard_example_miner {\n",
            "        num_hard_examples: 3000\n",
            "        iou_threshold: 0.99\n",
            "        loss_type: CLASSIFICATION\n",
            "        max_negatives_per_positive: 3\n",
            "        min_negatives_per_image: 3\n",
            "      }\n",
            "      classification_weight: 1.0\n",
            "      localization_weight: 1.0\n",
            "    }\n",
            "    normalize_loss_by_num_matches: true\n",
            "    post_processing {\n",
            "      batch_non_max_suppression {\n",
            "        score_threshold: 1e-8\n",
            "        iou_threshold: 0.6\n",
            "        max_detections_per_class: 100\n",
            "        max_total_detections: 100\n",
            "      }\n",
            "      score_converter: SIGMOID\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "train_config: {\n",
            "  batch_size: 24\n",
            "  optimizer {\n",
            "    rms_prop_optimizer: {\n",
            "      learning_rate: {\n",
            "        exponential_decay_learning_rate {\n",
            "          initial_learning_rate: 0.004\n",
            "          decay_steps: 800720\n",
            "          decay_factor: 0.95\n",
            "        }\n",
            "      }\n",
            "      momentum_optimizer_value: 0.9\n",
            "      decay: 0.9\n",
            "      epsilon: 1.0\n",
            "    }\n",
            "  }\n",
            "  fine_tune_checkpoint: \"/content/drive/My Drive/objectDetection/models/research/pretrained_model/model.ckpt\"\n",
            "  fine_tune_checkpoint_type:  \"detection\"\n",
            "  # Note: The below line limits the training process to 200K steps, which we\n",
            "  # empirically found to be sufficient enough to train the pets dataset. This\n",
            "  # effectively bypasses the learning rate schedule (the learning rate will\n",
            "  # never decay). Remove the below line to train indefinitely.\n",
            "  num_steps: 200000\n",
            "  data_augmentation_options {\n",
            "    random_horizontal_flip {\n",
            "    }\n",
            "  }\n",
            "  data_augmentation_options {\n",
            "    ssd_random_crop {\n",
            "    }\n",
            "  }\n",
            "\n",
            "   data_augmentation_options {\n",
            "    random_adjust_contrast {\n",
            "    }\n",
            "  }\n",
            "   data_augmentation_options {\n",
            "    random_rgb_to_gray {\n",
            "    }\n",
            "  }\n",
            "   data_augmentation_options {\n",
            "    random_vertical_flip {\n",
            "    }\n",
            "  }\n",
            "   data_augmentation_options {\n",
            "    random_rotation90 {\n",
            "    }\n",
            "  }\n",
            "   data_augmentation_options {\n",
            "    random_patch_gaussian {\n",
            "    }\n",
            "  }\n",
            "}\n",
            "train_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/drive/My Drive/objectDetection/train.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/drive/My Drive/objectDetection/label_map.pbtxt\"\n",
            "}\n",
            "\n",
            "eval_config: {\n",
            "  num_examples: 50\n",
            "  num_visualizations: 20\n",
            "}\n",
            "\n",
            "eval_input_reader: {\n",
            "  tf_record_input_reader {\n",
            "    input_path: \"/content/drive/My Drive/objectDetection/test.record\"\n",
            "  }\n",
            "  label_map_path: \"/content/drive/My Drive/objectDetection/label_map.pbtxt\"\n",
            "  shuffle: false\n",
            "  num_readers: 1\n",
            "}"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkXmGV18mlrc",
        "colab_type": "code",
        "outputId": "4d147540-6586-40e9-8c71-fdd3ed410c50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#path to the config file\n",
        "%%writefile object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n",
        "\n",
        "\n",
        "# SSD with Mobilenet v2 configuration for MSCOCO Dataset.\n",
        "# Users should configure the fine_tune_checkpoint field in the train config as\n",
        "# well as the label_map_path and input_path fields in the train_input_reader and\n",
        "# eval_input_reader. Search for \"PATH_TO_BE_CONFIGURED\" to find the fields that\n",
        "# should be configured.\n",
        "\n",
        "model {\n",
        "  ssd {\n",
        "    num_classes: 1\n",
        "    box_coder {\n",
        "      faster_rcnn_box_coder {\n",
        "        y_scale: 10.0\n",
        "        x_scale: 10.0\n",
        "        height_scale: 5.0\n",
        "        width_scale: 5.0\n",
        "      }\n",
        "    }\n",
        "    matcher {\n",
        "      argmax_matcher {\n",
        "        matched_threshold: 0.5\n",
        "        unmatched_threshold: 0.5\n",
        "        ignore_thresholds: false\n",
        "        negatives_lower_than_unmatched: true\n",
        "        force_match_for_each_row: true\n",
        "      }\n",
        "    }\n",
        "    similarity_calculator {\n",
        "      iou_similarity {\n",
        "      }\n",
        "    }\n",
        "    anchor_generator {\n",
        "      ssd_anchor_generator {\n",
        "        num_layers: 6\n",
        "        min_scale: 0.2\n",
        "        max_scale: 0.95\n",
        "        aspect_ratios: 1.0\n",
        "        aspect_ratios: 2.0\n",
        "        aspect_ratios: 0.5\n",
        "        aspect_ratios: 3.0\n",
        "        aspect_ratios: 0.3333\n",
        "      }\n",
        "    }\n",
        "    image_resizer {\n",
        "      fixed_shape_resizer {\n",
        "        height: 300\n",
        "        width: 300\n",
        "      }\n",
        "    }\n",
        "    box_predictor {\n",
        "      convolutional_box_predictor {\n",
        "        min_depth: 0\n",
        "        max_depth: 0\n",
        "        num_layers_before_predictor: 0\n",
        "        use_dropout: true\n",
        "        dropout_keep_probability: 0.8\n",
        "        kernel_size: 1\n",
        "        box_code_size: 4\n",
        "        apply_sigmoid_to_scores: false\n",
        "        conv_hyperparams {\n",
        "          activation: RELU_6,\n",
        "          regularizer {\n",
        "            l2_regularizer {\n",
        "              weight: 0.00004\n",
        "            }\n",
        "          }\n",
        "          initializer {\n",
        "            truncated_normal_initializer {\n",
        "              stddev: 0.03\n",
        "              mean: 0.0\n",
        "            }\n",
        "          }\n",
        "          batch_norm {\n",
        "            train: true,\n",
        "            scale: true,\n",
        "            center: true,\n",
        "            decay: 0.9997,\n",
        "            epsilon: 0.001,\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    feature_extractor {\n",
        "      type: 'ssd_mobilenet_v2'\n",
        "      min_depth: 16\n",
        "      depth_multiplier: 1.0\n",
        "      conv_hyperparams {\n",
        "        activation: RELU_6,\n",
        "        regularizer {\n",
        "          l2_regularizer {\n",
        "            weight: 0.00004\n",
        "          }\n",
        "        }\n",
        "        initializer {\n",
        "          truncated_normal_initializer {\n",
        "            stddev: 0.03\n",
        "            mean: 0.0\n",
        "          }\n",
        "        }\n",
        "        batch_norm {\n",
        "          train: true,\n",
        "          scale: true,\n",
        "          center: true,\n",
        "          decay: 0.9997,\n",
        "          epsilon: 0.001,\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    loss {\n",
        "      classification_loss {\n",
        "        weighted_sigmoid {\n",
        "        }\n",
        "      }\n",
        "      localization_loss {\n",
        "        weighted_smooth_l1 {\n",
        "        }\n",
        "      }\n",
        "      hard_example_miner {\n",
        "        num_hard_examples: 3000\n",
        "        iou_threshold: 0.99\n",
        "        loss_type: CLASSIFICATION\n",
        "        max_negatives_per_positive: 3\n",
        "        min_negatives_per_image: 3\n",
        "      }\n",
        "      classification_weight: 1.0\n",
        "      localization_weight: 1.0\n",
        "    }\n",
        "    normalize_loss_by_num_matches: true\n",
        "    post_processing {\n",
        "      batch_non_max_suppression {\n",
        "        score_threshold: 1e-8\n",
        "        iou_threshold: 0.6\n",
        "        max_detections_per_class: 100\n",
        "        max_total_detections: 100\n",
        "      }\n",
        "      score_converter: SIGMOID\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "train_config: {\n",
        "  batch_size: 24\n",
        "  optimizer {\n",
        "    rms_prop_optimizer: {\n",
        "      learning_rate: {\n",
        "        exponential_decay_learning_rate {\n",
        "          initial_learning_rate: 0.004\n",
        "          decay_steps: 800720\n",
        "          decay_factor: 0.95\n",
        "        }\n",
        "      }\n",
        "      momentum_optimizer_value: 0.9\n",
        "      decay: 0.9\n",
        "      epsilon: 1.0\n",
        "    }\n",
        "  }\n",
        "  fine_tune_checkpoint: \"/content/drive/My Drive/objectDetection/models/research/pretrained_model/model.ckpt\"\n",
        "  fine_tune_checkpoint_type:  \"detection\"\n",
        "  # Note: The below line limits the training process to 200K steps, which we\n",
        "  # empirically found to be sufficient enough to train the pets dataset. This\n",
        "  # effectively bypasses the learning rate schedule (the learning rate will\n",
        "  # never decay). Remove the below line to train indefinitely.\n",
        "  num_steps: 200000\n",
        "  data_augmentation_options {\n",
        "    random_horizontal_flip {\n",
        "    }\n",
        "  }\n",
        "  data_augmentation_options {\n",
        "    ssd_random_crop {\n",
        "    }\n",
        "  }\n",
        "\n",
        "   data_augmentation_options {\n",
        "    random_adjust_contrast {\n",
        "    }\n",
        "  }\n",
        "   data_augmentation_options {\n",
        "    random_rgb_to_gray {\n",
        "    }\n",
        "  }\n",
        "\n",
        "   data_augmentation_options {\n",
        "    random_patch_gaussian {\n",
        "    }\n",
        "  }\n",
        "}\n",
        "train_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/drive/My Drive/objectDetection/train.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/drive/My Drive/objectDetection/label_map.pbtxt\"\n",
        "}\n",
        "\n",
        "eval_config: {\n",
        "  num_examples: 1\n",
        "  num_visualizations: 1\n",
        "}\n",
        "\n",
        "eval_input_reader: {\n",
        "  tf_record_input_reader {\n",
        "    input_path: \"/content/drive/My Drive/objectDetection/test.record\"\n",
        "  }\n",
        "  label_map_path: \"/content/drive/My Drive/objectDetection/label_map.pbtxt\"\n",
        "  shuffle: false\n",
        "  num_readers: 1\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting object_detection/samples/configs/ssd_mobilenet_v2_coco.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8jPtgx_qRSg",
        "colab_type": "code",
        "outputId": "1449bbe7-9e7d-48be-c515-2263f4015503",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-01 12:56:29--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.70.189.149, 52.3.157.51, 52.70.195.119, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.70.189.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.4’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  5.14MB/s    in 2.6s    \n",
            "\n",
            "2020-04-01 12:56:33 (5.14 MB/s) - ‘ngrok-stable-linux-amd64.zip.4’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJBAfy4_rgBy",
        "colab_type": "code",
        "outputId": "a7ddd61f-b04a-4bc6-ffb2-6b9940ed0ccf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#the logs that are created while training \n",
        "LOG_DIR = \"training/\"\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "#The link to tensorboard.\n",
        "#works after the training starts.\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://968727f3.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMFB5z1M2mZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/objectDetection/models/research/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqHnsUB70UoK",
        "colab_type": "code",
        "outputId": "6b55e243-2d47-4c37-94d3-38964413fc40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python object_detection/legacy/train.py --logtostderr --train_dir=training/ --pipeline_config_path=object_detection/samples/configs/ssd_mobilenet_v2_coco.config"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:global step 33673: loss = 1.6212 (0.630 sec/step)\n",
            "I0401 17:52:01.494095 140457002706816 learning.py:507] global step 33673: loss = 1.6212 (0.630 sec/step)\n",
            "INFO:tensorflow:global step 33674: loss = 1.7483 (0.554 sec/step)\n",
            "I0401 17:52:02.049509 140457002706816 learning.py:507] global step 33674: loss = 1.7483 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 33675: loss = 1.0981 (0.595 sec/step)\n",
            "I0401 17:52:02.645245 140457002706816 learning.py:507] global step 33675: loss = 1.0981 (0.595 sec/step)\n",
            "INFO:tensorflow:global step 33676: loss = 1.6850 (0.605 sec/step)\n",
            "I0401 17:52:03.251830 140457002706816 learning.py:507] global step 33676: loss = 1.6850 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 33677: loss = 1.6329 (0.574 sec/step)\n",
            "I0401 17:52:03.827316 140457002706816 learning.py:507] global step 33677: loss = 1.6329 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 33678: loss = 2.3681 (0.667 sec/step)\n",
            "I0401 17:52:04.495778 140457002706816 learning.py:507] global step 33678: loss = 2.3681 (0.667 sec/step)\n",
            "INFO:tensorflow:global step 33679: loss = 1.6011 (0.580 sec/step)\n",
            "I0401 17:52:05.076988 140457002706816 learning.py:507] global step 33679: loss = 1.6011 (0.580 sec/step)\n",
            "INFO:tensorflow:global step 33680: loss = 1.9667 (0.581 sec/step)\n",
            "I0401 17:52:05.659794 140457002706816 learning.py:507] global step 33680: loss = 1.9667 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 33681: loss = 1.4004 (0.579 sec/step)\n",
            "I0401 17:52:06.240664 140457002706816 learning.py:507] global step 33681: loss = 1.4004 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 33682: loss = 1.6516 (0.587 sec/step)\n",
            "I0401 17:52:06.832266 140457002706816 learning.py:507] global step 33682: loss = 1.6516 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 33683: loss = 1.9430 (0.563 sec/step)\n",
            "I0401 17:52:07.397404 140457002706816 learning.py:507] global step 33683: loss = 1.9430 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 33684: loss = 1.4267 (0.553 sec/step)\n",
            "I0401 17:52:07.952375 140457002706816 learning.py:507] global step 33684: loss = 1.4267 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 33685: loss = 1.5447 (0.620 sec/step)\n",
            "I0401 17:52:08.574083 140457002706816 learning.py:507] global step 33685: loss = 1.5447 (0.620 sec/step)\n",
            "INFO:tensorflow:global step 33686: loss = 1.9776 (0.639 sec/step)\n",
            "I0401 17:52:09.215195 140457002706816 learning.py:507] global step 33686: loss = 1.9776 (0.639 sec/step)\n",
            "INFO:tensorflow:global step 33687: loss = 1.9145 (0.582 sec/step)\n",
            "I0401 17:52:09.798990 140457002706816 learning.py:507] global step 33687: loss = 1.9145 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 33688: loss = 1.3391 (0.602 sec/step)\n",
            "I0401 17:52:10.403067 140457002706816 learning.py:507] global step 33688: loss = 1.3391 (0.602 sec/step)\n",
            "INFO:tensorflow:global step 33689: loss = 1.4415 (0.566 sec/step)\n",
            "I0401 17:52:10.970639 140457002706816 learning.py:507] global step 33689: loss = 1.4415 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 33690: loss = 1.3454 (0.582 sec/step)\n",
            "I0401 17:52:11.554076 140457002706816 learning.py:507] global step 33690: loss = 1.3454 (0.582 sec/step)\n",
            "INFO:tensorflow:global step 33691: loss = 1.6355 (0.642 sec/step)\n",
            "I0401 17:52:12.197762 140457002706816 learning.py:507] global step 33691: loss = 1.6355 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 33692: loss = 1.4998 (0.565 sec/step)\n",
            "I0401 17:52:12.765136 140457002706816 learning.py:507] global step 33692: loss = 1.4998 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 33693: loss = 1.9012 (0.594 sec/step)\n",
            "I0401 17:52:13.361487 140457002706816 learning.py:507] global step 33693: loss = 1.9012 (0.594 sec/step)\n",
            "INFO:tensorflow:global step 33694: loss = 1.4382 (0.558 sec/step)\n",
            "I0401 17:52:13.920774 140457002706816 learning.py:507] global step 33694: loss = 1.4382 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 33695: loss = 2.2331 (0.648 sec/step)\n",
            "I0401 17:52:14.570958 140457002706816 learning.py:507] global step 33695: loss = 2.2331 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 33696: loss = 1.5345 (0.569 sec/step)\n",
            "I0401 17:52:15.142297 140457002706816 learning.py:507] global step 33696: loss = 1.5345 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 33697: loss = 1.2706 (0.597 sec/step)\n",
            "I0401 17:52:15.741660 140457002706816 learning.py:507] global step 33697: loss = 1.2706 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 33698: loss = 1.3836 (0.553 sec/step)\n",
            "I0401 17:52:16.296561 140457002706816 learning.py:507] global step 33698: loss = 1.3836 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 33699: loss = 1.5504 (0.569 sec/step)\n",
            "I0401 17:52:16.867671 140457002706816 learning.py:507] global step 33699: loss = 1.5504 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 33700: loss = 1.0296 (0.584 sec/step)\n",
            "I0401 17:52:17.452648 140457002706816 learning.py:507] global step 33700: loss = 1.0296 (0.584 sec/step)\n",
            "INFO:tensorflow:global step 33701: loss = 1.3954 (0.592 sec/step)\n",
            "I0401 17:52:18.046809 140457002706816 learning.py:507] global step 33701: loss = 1.3954 (0.592 sec/step)\n",
            "INFO:tensorflow:global step 33702: loss = 2.1135 (0.598 sec/step)\n",
            "I0401 17:52:18.646199 140457002706816 learning.py:507] global step 33702: loss = 2.1135 (0.598 sec/step)\n",
            "INFO:tensorflow:global step 33703: loss = 2.0247 (0.586 sec/step)\n",
            "I0401 17:52:19.233706 140457002706816 learning.py:507] global step 33703: loss = 2.0247 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 33704: loss = 2.3525 (0.548 sec/step)\n",
            "I0401 17:52:19.783019 140457002706816 learning.py:507] global step 33704: loss = 2.3525 (0.548 sec/step)\n",
            "INFO:tensorflow:global step 33705: loss = 1.5350 (0.560 sec/step)\n",
            "I0401 17:52:20.344951 140457002706816 learning.py:507] global step 33705: loss = 1.5350 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 33706: loss = 1.5492 (0.571 sec/step)\n",
            "I0401 17:52:20.917374 140457002706816 learning.py:507] global step 33706: loss = 1.5492 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 33707: loss = 1.4849 (0.569 sec/step)\n",
            "I0401 17:52:21.488190 140457002706816 learning.py:507] global step 33707: loss = 1.4849 (0.569 sec/step)\n",
            "INFO:tensorflow:global step 33708: loss = 2.3167 (0.621 sec/step)\n",
            "I0401 17:52:22.110920 140457002706816 learning.py:507] global step 33708: loss = 2.3167 (0.621 sec/step)\n",
            "INFO:tensorflow:global step 33709: loss = 1.4958 (0.605 sec/step)\n",
            "I0401 17:52:22.717329 140457002706816 learning.py:507] global step 33709: loss = 1.4958 (0.605 sec/step)\n",
            "INFO:tensorflow:global step 33710: loss = 1.8402 (0.641 sec/step)\n",
            "I0401 17:52:23.360049 140457002706816 learning.py:507] global step 33710: loss = 1.8402 (0.641 sec/step)\n",
            "INFO:tensorflow:global step 33711: loss = 1.4741 (0.623 sec/step)\n",
            "I0401 17:52:23.984689 140457002706816 learning.py:507] global step 33711: loss = 1.4741 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 33712: loss = 1.8951 (0.532 sec/step)\n",
            "I0401 17:52:24.518064 140457002706816 learning.py:507] global step 33712: loss = 1.8951 (0.532 sec/step)\n",
            "INFO:tensorflow:global step 33713: loss = 1.4298 (0.596 sec/step)\n",
            "I0401 17:52:25.114942 140457002706816 learning.py:507] global step 33713: loss = 1.4298 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 33714: loss = 1.8824 (0.600 sec/step)\n",
            "I0401 17:52:25.716519 140457002706816 learning.py:507] global step 33714: loss = 1.8824 (0.600 sec/step)\n",
            "INFO:tensorflow:global step 33715: loss = 1.8022 (0.561 sec/step)\n",
            "I0401 17:52:26.279737 140457002706816 learning.py:507] global step 33715: loss = 1.8022 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 33716: loss = 2.0616 (0.625 sec/step)\n",
            "I0401 17:52:26.906018 140457002706816 learning.py:507] global step 33716: loss = 2.0616 (0.625 sec/step)\n",
            "INFO:tensorflow:global step 33717: loss = 1.2685 (0.629 sec/step)\n",
            "I0401 17:52:27.536767 140457002706816 learning.py:507] global step 33717: loss = 1.2685 (0.629 sec/step)\n",
            "INFO:tensorflow:global step 33718: loss = 1.5851 (0.596 sec/step)\n",
            "I0401 17:52:28.133746 140457002706816 learning.py:507] global step 33718: loss = 1.5851 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 33719: loss = 1.7174 (0.550 sec/step)\n",
            "I0401 17:52:28.685169 140457002706816 learning.py:507] global step 33719: loss = 1.7174 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 33720: loss = 1.4250 (0.596 sec/step)\n",
            "I0401 17:52:29.282564 140457002706816 learning.py:507] global step 33720: loss = 1.4250 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 33721: loss = 1.8446 (0.571 sec/step)\n",
            "I0401 17:52:29.855959 140457002706816 learning.py:507] global step 33721: loss = 1.8446 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 33722: loss = 1.1745 (0.597 sec/step)\n",
            "I0401 17:52:30.455002 140457002706816 learning.py:507] global step 33722: loss = 1.1745 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 33723: loss = 2.3776 (0.617 sec/step)\n",
            "I0401 17:52:31.073065 140457002706816 learning.py:507] global step 33723: loss = 2.3776 (0.617 sec/step)\n",
            "INFO:tensorflow:global step 33724: loss = 1.3859 (0.642 sec/step)\n",
            "I0401 17:52:31.716466 140457002706816 learning.py:507] global step 33724: loss = 1.3859 (0.642 sec/step)\n",
            "INFO:tensorflow:global step 33725: loss = 1.6289 (0.564 sec/step)\n",
            "I0401 17:52:32.282558 140457002706816 learning.py:507] global step 33725: loss = 1.6289 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 33726: loss = 2.1280 (0.581 sec/step)\n",
            "I0401 17:52:32.864832 140457002706816 learning.py:507] global step 33726: loss = 2.1280 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 33727: loss = 1.6172 (0.590 sec/step)\n",
            "I0401 17:52:33.456741 140457002706816 learning.py:507] global step 33727: loss = 1.6172 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 33728: loss = 2.2385 (0.623 sec/step)\n",
            "I0401 17:52:34.081702 140457002706816 learning.py:507] global step 33728: loss = 2.2385 (0.623 sec/step)\n",
            "INFO:tensorflow:global step 33729: loss = 1.8685 (0.640 sec/step)\n",
            "I0401 17:52:34.723419 140457002706816 learning.py:507] global step 33729: loss = 1.8685 (0.640 sec/step)\n",
            "INFO:tensorflow:global step 33730: loss = 1.0068 (0.577 sec/step)\n",
            "I0401 17:52:35.301480 140457002706816 learning.py:507] global step 33730: loss = 1.0068 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 33731: loss = 1.4789 (0.563 sec/step)\n",
            "I0401 17:52:35.866007 140457002706816 learning.py:507] global step 33731: loss = 1.4789 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 33732: loss = 1.8147 (0.597 sec/step)\n",
            "I0401 17:52:36.465033 140457002706816 learning.py:507] global step 33732: loss = 1.8147 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 33733: loss = 1.4836 (0.558 sec/step)\n",
            "I0401 17:52:37.024337 140457002706816 learning.py:507] global step 33733: loss = 1.4836 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 33734: loss = 1.0682 (0.593 sec/step)\n",
            "I0401 17:52:37.618645 140457002706816 learning.py:507] global step 33734: loss = 1.0682 (0.593 sec/step)\n",
            "INFO:tensorflow:global step 33735: loss = 1.5228 (0.648 sec/step)\n",
            "I0401 17:52:38.268173 140457002706816 learning.py:507] global step 33735: loss = 1.5228 (0.648 sec/step)\n",
            "INFO:tensorflow:global step 33736: loss = 2.0861 (0.615 sec/step)\n",
            "I0401 17:52:38.884590 140457002706816 learning.py:507] global step 33736: loss = 2.0861 (0.615 sec/step)\n",
            "INFO:tensorflow:global step 33737: loss = 1.5733 (0.581 sec/step)\n",
            "I0401 17:52:39.466773 140457002706816 learning.py:507] global step 33737: loss = 1.5733 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 33738: loss = 2.2632 (0.613 sec/step)\n",
            "I0401 17:52:40.081366 140457002706816 learning.py:507] global step 33738: loss = 2.2632 (0.613 sec/step)\n",
            "INFO:tensorflow:global step 33739: loss = 1.5907 (0.554 sec/step)\n",
            "I0401 17:52:40.637142 140457002706816 learning.py:507] global step 33739: loss = 1.5907 (0.554 sec/step)\n",
            "INFO:tensorflow:global step 33740: loss = 1.9100 (0.560 sec/step)\n",
            "I0401 17:52:41.200294 140457002706816 learning.py:507] global step 33740: loss = 1.9100 (0.560 sec/step)\n",
            "INFO:tensorflow:global step 33741: loss = 2.0760 (0.626 sec/step)\n",
            "I0401 17:52:41.827358 140457002706816 learning.py:507] global step 33741: loss = 2.0760 (0.626 sec/step)\n",
            "INFO:tensorflow:global step 33742: loss = 1.5105 (0.565 sec/step)\n",
            "I0401 17:52:42.394046 140457002706816 learning.py:507] global step 33742: loss = 1.5105 (0.565 sec/step)\n",
            "INFO:tensorflow:global step 33743: loss = 1.8138 (0.542 sec/step)\n",
            "I0401 17:52:42.937634 140457002706816 learning.py:507] global step 33743: loss = 1.8138 (0.542 sec/step)\n",
            "INFO:tensorflow:global step 33744: loss = 2.2124 (0.573 sec/step)\n",
            "I0401 17:52:43.512630 140457002706816 learning.py:507] global step 33744: loss = 2.2124 (0.573 sec/step)\n",
            "INFO:tensorflow:global step 33745: loss = 2.2755 (0.559 sec/step)\n",
            "I0401 17:52:44.072947 140457002706816 learning.py:507] global step 33745: loss = 2.2755 (0.559 sec/step)\n",
            "INFO:tensorflow:global step 33746: loss = 1.8911 (0.564 sec/step)\n",
            "I0401 17:52:44.638206 140457002706816 learning.py:507] global step 33746: loss = 1.8911 (0.564 sec/step)\n",
            "INFO:tensorflow:global step 33747: loss = 2.0565 (0.579 sec/step)\n",
            "I0401 17:52:45.218969 140457002706816 learning.py:507] global step 33747: loss = 2.0565 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 33748: loss = 2.5208 (0.622 sec/step)\n",
            "I0401 17:52:45.842781 140457002706816 learning.py:507] global step 33748: loss = 2.5208 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 33749: loss = 1.4593 (0.599 sec/step)\n",
            "I0401 17:52:46.443159 140457002706816 learning.py:507] global step 33749: loss = 1.4593 (0.599 sec/step)\n",
            "INFO:tensorflow:global step 33750: loss = 1.6142 (0.581 sec/step)\n",
            "I0401 17:52:47.025376 140457002706816 learning.py:507] global step 33750: loss = 1.6142 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 33751: loss = 1.4655 (0.556 sec/step)\n",
            "I0401 17:52:47.582713 140457002706816 learning.py:507] global step 33751: loss = 1.4655 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 33752: loss = 1.5189 (0.562 sec/step)\n",
            "I0401 17:52:48.146861 140457002706816 learning.py:507] global step 33752: loss = 1.5189 (0.562 sec/step)\n",
            "INFO:tensorflow:global step 33753: loss = 2.0262 (0.586 sec/step)\n",
            "I0401 17:52:48.734214 140457002706816 learning.py:507] global step 33753: loss = 2.0262 (0.586 sec/step)\n",
            "INFO:tensorflow:global step 33754: loss = 1.9052 (0.578 sec/step)\n",
            "I0401 17:52:49.313411 140457002706816 learning.py:507] global step 33754: loss = 1.9052 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 33755: loss = 1.4647 (0.587 sec/step)\n",
            "I0401 17:52:49.902313 140457002706816 learning.py:507] global step 33755: loss = 1.4647 (0.587 sec/step)\n",
            "INFO:tensorflow:global step 33756: loss = 1.9292 (0.571 sec/step)\n",
            "I0401 17:52:50.475117 140457002706816 learning.py:507] global step 33756: loss = 1.9292 (0.571 sec/step)\n",
            "INFO:tensorflow:global step 33757: loss = 1.7015 (0.577 sec/step)\n",
            "I0401 17:52:51.054216 140457002706816 learning.py:507] global step 33757: loss = 1.7015 (0.577 sec/step)\n",
            "INFO:tensorflow:global step 33758: loss = 1.8153 (0.601 sec/step)\n",
            "I0401 17:52:51.657188 140457002706816 learning.py:507] global step 33758: loss = 1.8153 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 33759: loss = 1.8158 (0.553 sec/step)\n",
            "I0401 17:52:52.213760 140457002706816 learning.py:507] global step 33759: loss = 1.8158 (0.553 sec/step)\n",
            "INFO:tensorflow:global step 33760: loss = 1.8648 (0.556 sec/step)\n",
            "I0401 17:52:52.772198 140457002706816 learning.py:507] global step 33760: loss = 1.8648 (0.556 sec/step)\n",
            "INFO:tensorflow:global step 33761: loss = 1.9720 (0.567 sec/step)\n",
            "I0401 17:52:53.340753 140457002706816 learning.py:507] global step 33761: loss = 1.9720 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 33762: loss = 1.3481 (0.622 sec/step)\n",
            "I0401 17:52:53.964246 140457002706816 learning.py:507] global step 33762: loss = 1.3481 (0.622 sec/step)\n",
            "INFO:tensorflow:global step 33763: loss = 2.1260 (0.588 sec/step)\n",
            "I0401 17:52:54.554114 140457002706816 learning.py:507] global step 33763: loss = 2.1260 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 33764: loss = 1.7108 (0.568 sec/step)\n",
            "I0401 17:52:55.369266 140457002706816 learning.py:507] global step 33764: loss = 1.7108 (0.568 sec/step)\n",
            "INFO:tensorflow:Recording summary at step 33764.\n",
            "I0401 17:52:55.838974 140453281855232 supervisor.py:1050] Recording summary at step 33764.\n",
            "INFO:tensorflow:global step 33765: loss = 1.7615 (0.804 sec/step)\n",
            "I0401 17:52:56.176441 140457002706816 learning.py:507] global step 33765: loss = 1.7615 (0.804 sec/step)\n",
            "INFO:tensorflow:global step 33766: loss = 2.2165 (0.612 sec/step)\n",
            "I0401 17:52:56.789431 140457002706816 learning.py:507] global step 33766: loss = 2.2165 (0.612 sec/step)\n",
            "INFO:tensorflow:global step 33767: loss = 2.1561 (0.531 sec/step)\n",
            "I0401 17:52:57.322298 140457002706816 learning.py:507] global step 33767: loss = 2.1561 (0.531 sec/step)\n",
            "INFO:tensorflow:global step 33768: loss = 1.6708 (0.547 sec/step)\n",
            "I0401 17:52:57.871265 140457002706816 learning.py:507] global step 33768: loss = 1.6708 (0.547 sec/step)\n",
            "INFO:tensorflow:global step 33769: loss = 2.2320 (0.658 sec/step)\n",
            "I0401 17:52:58.531187 140457002706816 learning.py:507] global step 33769: loss = 2.2320 (0.658 sec/step)\n",
            "INFO:tensorflow:global step 33770: loss = 1.6823 (0.561 sec/step)\n",
            "I0401 17:52:59.094299 140457002706816 learning.py:507] global step 33770: loss = 1.6823 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 33771: loss = 2.0531 (0.601 sec/step)\n",
            "I0401 17:52:59.696572 140457002706816 learning.py:507] global step 33771: loss = 2.0531 (0.601 sec/step)\n",
            "INFO:tensorflow:global step 33772: loss = 1.8735 (0.578 sec/step)\n",
            "I0401 17:53:00.275553 140457002706816 learning.py:507] global step 33772: loss = 1.8735 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 33773: loss = 1.3156 (0.579 sec/step)\n",
            "I0401 17:53:00.856016 140457002706816 learning.py:507] global step 33773: loss = 1.3156 (0.579 sec/step)\n",
            "INFO:tensorflow:global step 33774: loss = 1.4013 (0.574 sec/step)\n",
            "I0401 17:53:01.431760 140457002706816 learning.py:507] global step 33774: loss = 1.4013 (0.574 sec/step)\n",
            "INFO:tensorflow:global step 33775: loss = 1.7957 (0.597 sec/step)\n",
            "I0401 17:53:02.030209 140457002706816 learning.py:507] global step 33775: loss = 1.7957 (0.597 sec/step)\n",
            "INFO:tensorflow:global step 33776: loss = 1.6509 (0.596 sec/step)\n",
            "I0401 17:53:02.627954 140457002706816 learning.py:507] global step 33776: loss = 1.6509 (0.596 sec/step)\n",
            "INFO:tensorflow:global step 33777: loss = 1.9891 (0.581 sec/step)\n",
            "I0401 17:53:03.211119 140457002706816 learning.py:507] global step 33777: loss = 1.9891 (0.581 sec/step)\n",
            "INFO:tensorflow:global step 33778: loss = 2.0214 (0.578 sec/step)\n",
            "I0401 17:53:03.790223 140457002706816 learning.py:507] global step 33778: loss = 2.0214 (0.578 sec/step)\n",
            "INFO:tensorflow:global step 33779: loss = 1.2881 (0.583 sec/step)\n",
            "I0401 17:53:04.374666 140457002706816 learning.py:507] global step 33779: loss = 1.2881 (0.583 sec/step)\n",
            "INFO:tensorflow:global step 33780: loss = 2.1931 (0.590 sec/step)\n",
            "I0401 17:53:04.966772 140457002706816 learning.py:507] global step 33780: loss = 2.1931 (0.590 sec/step)\n",
            "INFO:tensorflow:global step 33781: loss = 1.2633 (0.563 sec/step)\n",
            "I0401 17:53:05.531435 140457002706816 learning.py:507] global step 33781: loss = 1.2633 (0.563 sec/step)\n",
            "INFO:tensorflow:global step 33782: loss = 1.6249 (0.585 sec/step)\n",
            "I0401 17:53:06.117409 140457002706816 learning.py:507] global step 33782: loss = 1.6249 (0.585 sec/step)\n",
            "INFO:tensorflow:global step 33783: loss = 2.3073 (0.561 sec/step)\n",
            "I0401 17:53:06.680679 140457002706816 learning.py:507] global step 33783: loss = 2.3073 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 33784: loss = 1.4334 (0.588 sec/step)\n",
            "I0401 17:53:07.270101 140457002706816 learning.py:507] global step 33784: loss = 1.4334 (0.588 sec/step)\n",
            "INFO:tensorflow:global step 33785: loss = 1.6023 (0.545 sec/step)\n",
            "I0401 17:53:07.816201 140457002706816 learning.py:507] global step 33785: loss = 1.6023 (0.545 sec/step)\n",
            "INFO:tensorflow:global step 33786: loss = 1.7122 (0.570 sec/step)\n",
            "I0401 17:53:08.387973 140457002706816 learning.py:507] global step 33786: loss = 1.7122 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 33787: loss = 1.7550 (0.759 sec/step)\n",
            "I0401 17:53:09.148452 140457002706816 learning.py:507] global step 33787: loss = 1.7550 (0.759 sec/step)\n",
            "INFO:tensorflow:global step 33788: loss = 2.2718 (0.537 sec/step)\n",
            "I0401 17:53:09.687255 140457002706816 learning.py:507] global step 33788: loss = 2.2718 (0.537 sec/step)\n",
            "INFO:tensorflow:global step 33789: loss = 2.1066 (0.558 sec/step)\n",
            "I0401 17:53:10.247761 140457002706816 learning.py:507] global step 33789: loss = 2.1066 (0.558 sec/step)\n",
            "INFO:tensorflow:global step 33790: loss = 1.7315 (0.567 sec/step)\n",
            "I0401 17:53:10.816419 140457002706816 learning.py:507] global step 33790: loss = 1.7315 (0.567 sec/step)\n",
            "INFO:tensorflow:global step 33791: loss = 1.6675 (0.570 sec/step)\n",
            "I0401 17:53:11.387606 140457002706816 learning.py:507] global step 33791: loss = 1.6675 (0.570 sec/step)\n",
            "INFO:tensorflow:global step 33792: loss = 1.4532 (0.566 sec/step)\n",
            "I0401 17:53:11.954951 140457002706816 learning.py:507] global step 33792: loss = 1.4532 (0.566 sec/step)\n",
            "INFO:tensorflow:global step 33793: loss = 1.8132 (0.591 sec/step)\n",
            "I0401 17:53:12.547783 140457002706816 learning.py:507] global step 33793: loss = 1.8132 (0.591 sec/step)\n",
            "INFO:tensorflow:global step 33794: loss = 1.6800 (0.614 sec/step)\n",
            "I0401 17:53:13.163233 140457002706816 learning.py:507] global step 33794: loss = 1.6800 (0.614 sec/step)\n",
            "INFO:tensorflow:global step 33795: loss = 1.3400 (0.561 sec/step)\n",
            "I0401 17:53:13.725830 140457002706816 learning.py:507] global step 33795: loss = 1.3400 (0.561 sec/step)\n",
            "INFO:tensorflow:global step 33796: loss = 1.7593 (0.568 sec/step)\n",
            "I0401 17:53:14.295617 140457002706816 learning.py:507] global step 33796: loss = 1.7593 (0.568 sec/step)\n",
            "INFO:tensorflow:global step 33797: loss = 1.4265 (0.550 sec/step)\n",
            "I0401 17:53:14.847121 140457002706816 learning.py:507] global step 33797: loss = 1.4265 (0.550 sec/step)\n",
            "INFO:tensorflow:global step 33798: loss = 2.3222 (0.576 sec/step)\n",
            "I0401 17:53:15.424513 140457002706816 learning.py:507] global step 33798: loss = 2.3222 (0.576 sec/step)\n",
            "INFO:tensorflow:global step 33799: loss = 1.9650 (0.560 sec/step)\n",
            "I0401 17:53:15.986158 140457002706816 learning.py:507] global step 33799: loss = 1.9650 (0.560 sec/step)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhC4m6ddwsUN",
        "colab_type": "code",
        "outputId": "d53c44a7-c7e5-4dc0-a386-1c1305519c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python object_detection/model_main.py \\\n",
        "    --pipeline_config_path=/content/drive/My\\ Drive/objectDetection/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config \\\n",
        "    --model_dir=training/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From object_detection/model_main.py:109: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0331 20:13:01.277819 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/config_util.py:102: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "W0331 20:13:01.283674 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:628: The name tf.logging.warning is deprecated. Please use tf.compat.v1.logging.warning instead.\n",
            "\n",
            "WARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\n",
            "W0331 20:13:01.283902 140445722748800 model_lib.py:629] Forced number of epochs for all eval validations to be 1.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0331 20:13:01.284097 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/config_util.py:488: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:Maybe overwriting train_steps: None\n",
            "I0331 20:13:01.284227 140445722748800 config_util.py:488] Maybe overwriting train_steps: None\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I0331 20:13:01.284370 140445722748800 config_util.py:488] Maybe overwriting use_bfloat16: False\n",
            "INFO:tensorflow:Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "I0331 20:13:01.284485 140445722748800 config_util.py:488] Maybe overwriting sample_1_of_n_eval_examples: 1\n",
            "INFO:tensorflow:Maybe overwriting eval_num_epochs: 1\n",
            "I0331 20:13:01.284605 140445722748800 config_util.py:488] Maybe overwriting eval_num_epochs: 1\n",
            "INFO:tensorflow:Maybe overwriting load_pretrained: True\n",
            "I0331 20:13:01.284720 140445722748800 config_util.py:488] Maybe overwriting load_pretrained: True\n",
            "INFO:tensorflow:Ignoring config override key: load_pretrained\n",
            "I0331 20:13:01.284851 140445722748800 config_util.py:498] Ignoring config override key: load_pretrained\n",
            "WARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "W0331 20:13:01.285568 140445722748800 model_lib.py:645] Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\n",
            "INFO:tensorflow:create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "I0331 20:13:01.285726 140445722748800 model_lib.py:680] create_estimator_and_inputs: use_tpu False, export_to_tpu False\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbb997ec160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "I0331 20:13:01.286328 140445722748800 estimator.py:212] Using config: {'_model_dir': 'training/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbb997ec160>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "WARNING:tensorflow:Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7fbb997e5730>) includes params argument, but params are not passed to Estimator.\n",
            "W0331 20:13:01.286636 140445722748800 model_fn.py:630] Estimator's model_fn (<function create_model_fn.<locals>.model_fn at 0x7fbb997e5730>) includes params argument, but params are not passed to Estimator.\n",
            "INFO:tensorflow:Not using Distribute Coordinator.\n",
            "I0331 20:13:01.287537 140445722748800 estimator_training.py:186] Not using Distribute Coordinator.\n",
            "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
            "I0331 20:13:01.287754 140445722748800 training.py:612] Running training and evaluation locally (non-distributed).\n",
            "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "I0331 20:13:01.288061 140445722748800 training.py:700] Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 600.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "W0331 20:13:01.305871 140445722748800 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "W0331 20:13:01.318469 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/data_decoders/tf_example_decoder.py:182: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "W0331 20:13:01.318894 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/data_decoders/tf_example_decoder.py:197: The name tf.VarLenFeature is deprecated. Please use tf.io.VarLenFeature instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0331 20:13:01.338659 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:64: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W0331 20:13:01.340667 140445722748800 dataset_builder.py:72] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "W0331 20:13:01.348431 140445722748800 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.experimental.parallel_interleave(...)`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "W0331 20:13:01.348587 140445722748800 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/data/python/ops/interleave_ops.py:77: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.experimental.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.experimental_determinstic`.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W0331 20:13:01.372938 140445722748800 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:155: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.\n",
            "\n",
            "W0331 20:13:02.779389 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.logging.warn is deprecated. Please use tf.compat.v1.logging.warn instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "W0331 20:13:10.668582 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.is_nan is deprecated. Please use tf.math.is_nan instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0331 20:13:10.755758 140445722748800 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/ops.py:493: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0331 20:13:13.083063 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W0331 20:13:16.642706 140445722748800 api.py:332] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/operators/control_flow.py:1004: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/core/preprocessor.py:244: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "W0331 20:13:20.057040 140445722748800 deprecation.py:506] From /content/drive/My Drive/objectDetection/models/research/object_detection/core/preprocessor.py:244: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "W0331 20:13:20.860160 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.debugging.assert_greater_equal is deprecated. Please use tf.compat.v1.debugging.assert_greater_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.debugging.assert_less is deprecated. Please use tf.compat.v1.debugging.assert_less instead.\n",
            "\n",
            "W0331 20:13:20.860636 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.debugging.assert_less is deprecated. Please use tf.compat.v1.debugging.assert_less instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
            "\n",
            "W0331 20:13:20.861405 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.ceil is deprecated. Please use tf.math.ceil instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0331 20:13:22.201180 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "W0331 20:13:22.202357 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/inputs.py:166: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0331 20:13:22.665999 140445722748800 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/inputs.py:166: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\n",
            "\n",
            "W0331 20:13:24.563210 140445722748800 module_wrapper.py:139] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.string_to_hash_bucket_fast is deprecated. Please use tf.strings.to_hash_bucket_fast instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "W0331 20:13:25.108641 140445722748800 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0331 20:13:25.125051 140445722748800 estimator.py:1148] Calling model_fn.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:589: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0331 20:13:25.545977 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:589: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0331 20:13:25.546276 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0331 20:13:25.550190 140445722748800 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0331 20:13:28.911998 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:13:28.926360 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:13:28.976234 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:13:29.033736 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:13:29.086104 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:13:29.139895 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:13:29.196871 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W0331 20:13:29.255366 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/variables_helper.py:179: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0331 20:13:29.256513 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/variables_helper.py:139: The name tf.train.NewCheckpointReader is deprecated. Please use tf.compat.v1.train.NewCheckpointReader instead.\n",
            "\n",
            "W0331 20:13:29.263919 140445722748800 variables_helper.py:154] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 512]], model variable shape: [[3, 3, 256, 512]]. This variable will not be initialized from the checkpoint.\n",
            "W0331 20:13:29.264117 140445722748800 variables_helper.py:154] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0331 20:13:29.264307 140445722748800 variables_helper.py:154] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 256]], model variable shape: [[3, 3, 128, 256]]. This variable will not be initialized from the checkpoint.\n",
            "W0331 20:13:29.264575 140445722748800 variables_helper.py:154] Variable [FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 64, 128]], model variable shape: [[3, 3, 64, 128]]. This variable will not be initialized from the checkpoint.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:353: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "W0331 20:13:29.265075 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:353: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0331 20:13:30.456026 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/box_coders/faster_rcnn_box_coder.py:82: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1163: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0331 20:13:35.119751 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1163: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "W0331 20:13:35.127118 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/core/losses.py:177: The name tf.losses.huber_loss is deprecated. Please use tf.compat.v1.losses.huber_loss instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "W0331 20:13:35.128632 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/core/losses.py:183: The name tf.losses.Reduction is deprecated. Please use tf.compat.v1.losses.Reduction instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1275: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0331 20:13:35.832404 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:1275: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "W0331 20:13:35.835265 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:380: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "W0331 20:13:35.835586 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/learning_schedules.py:66: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "W0331 20:13:35.846148 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/builders/optimizer_builder.py:47: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:398: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "W0331 20:13:35.846410 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:398: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0331 20:13:38.605598 140445722748800 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0331 20:13:45.604703 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:515: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0331 20:13:46.459983 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:519: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
            "\n",
            "W0331 20:13:46.460293 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:520: The name tf.train.Scaffold is deprecated. Please use tf.compat.v1.train.Scaffold instead.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0331 20:13:46.460945 140445722748800 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "I0331 20:13:46.462564 140445722748800 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0331 20:13:51.132102 140445722748800 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-31 20:13:51.132570: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-03-31 20:13:51.137472: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-03-31 20:13:51.137805: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c152c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-31 20:13:51.137870: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-03-31 20:13:51.140105: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-03-31 20:13:51.213738: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:13:51.214585: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2c15b80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-31 20:13:51.214617: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2020-03-31 20:13:51.214894: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:13:51.215525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-31 20:13:51.215885: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-31 20:13:51.217110: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-03-31 20:13:51.218311: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-03-31 20:13:51.218861: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-03-31 20:13:51.220803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-03-31 20:13:51.222032: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-03-31 20:13:51.225620: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-31 20:13:51.225732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:13:51.226459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:13:51.227091: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-31 20:13:51.227189: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-31 20:13:51.228570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-31 20:13:51.228614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-31 20:13:51.228626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-31 20:13:51.228793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:13:51.229494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:13:51.230199: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-03-31 20:13:51.230238: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-4128\n",
            "I0331 20:13:51.234515 140445722748800 saver.py:1284] Restoring parameters from training/model.ckpt-4128\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "W0331 20:13:53.730371 140445722748800 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0331 20:13:54.981763 140445722748800 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0331 20:13:55.402194 140445722748800 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 4128 into training/model.ckpt.\n",
            "I0331 20:14:08.020370 140445722748800 basic_session_run_hooks.py:606] Saving checkpoints for 4128 into training/model.ckpt.\n",
            "2020-03-31 20:14:20.459012: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-31 20:14:21.455622: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "INFO:tensorflow:loss = 2.5603824, step = 4128\n",
            "I0331 20:14:24.331495 140445722748800 basic_session_run_hooks.py:262] loss = 2.5603824, step = 4128\n",
            "INFO:tensorflow:global_step/sec: 1.18052\n",
            "I0331 20:15:49.039160 140445722748800 basic_session_run_hooks.py:692] global_step/sec: 1.18052\n",
            "INFO:tensorflow:loss = 2.9434102, step = 4228 (84.710 sec)\n",
            "I0331 20:15:49.041907 140445722748800 basic_session_run_hooks.py:260] loss = 2.9434102, step = 4228 (84.710 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.29388\n",
            "I0331 20:17:06.326062 140445722748800 basic_session_run_hooks.py:692] global_step/sec: 1.29388\n",
            "INFO:tensorflow:loss = 3.6332536, step = 4328 (77.285 sec)\n",
            "I0331 20:17:06.327166 140445722748800 basic_session_run_hooks.py:260] loss = 3.6332536, step = 4328 (77.285 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.27877\n",
            "I0331 20:18:24.526519 140445722748800 basic_session_run_hooks.py:692] global_step/sec: 1.27877\n",
            "INFO:tensorflow:loss = 3.679783, step = 4428 (78.201 sec)\n",
            "I0331 20:18:24.528596 140445722748800 basic_session_run_hooks.py:260] loss = 3.679783, step = 4428 (78.201 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.30597\n",
            "I0331 20:19:41.098298 140445722748800 basic_session_run_hooks.py:692] global_step/sec: 1.30597\n",
            "INFO:tensorflow:loss = 3.3970032, step = 4528 (76.571 sec)\n",
            "I0331 20:19:41.099509 140445722748800 basic_session_run_hooks.py:260] loss = 3.3970032, step = 4528 (76.571 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.28158\n",
            "I0331 20:20:59.126920 140445722748800 basic_session_run_hooks.py:692] global_step/sec: 1.28158\n",
            "INFO:tensorflow:loss = 3.0431988, step = 4628 (78.029 sec)\n",
            "I0331 20:20:59.128644 140445722748800 basic_session_run_hooks.py:260] loss = 3.0431988, step = 4628 (78.029 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.3097\n",
            "I0331 20:22:15.480128 140445722748800 basic_session_run_hooks.py:692] global_step/sec: 1.3097\n",
            "INFO:tensorflow:loss = 2.9851673, step = 4728 (76.352 sec)\n",
            "I0331 20:22:15.481121 140445722748800 basic_session_run_hooks.py:260] loss = 2.9851673, step = 4728 (76.352 sec)\n",
            "INFO:tensorflow:global_step/sec: 1.2757\n",
            "I0331 20:23:33.868331 140445722748800 basic_session_run_hooks.py:692] global_step/sec: 1.2757\n",
            "INFO:tensorflow:loss = 2.4749718, step = 4828 (78.389 sec)\n",
            "I0331 20:23:33.870436 140445722748800 basic_session_run_hooks.py:260] loss = 2.4749718, step = 4828 (78.389 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 4878 into training/model.ckpt.\n",
            "I0331 20:24:11.032969 140445722748800 basic_session_run_hooks.py:606] Saving checkpoints for 4878 into training/model.ckpt.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "W0331 20:24:11.288448 140445722748800 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "I0331 20:24:13.960978 140445722748800 estimator.py:1148] Calling model_fn.\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:24:16.906986 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:24:16.950137 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:24:16.992328 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:24:17.036202 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:24:17.078493 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0331 20:24:17.124631 140445722748800 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/eval_util.py:796: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W0331 20:24:17.997183 140445722748800 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/eval_util.py:796: to_int64 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/visualization_utils.py:498: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "W0331 20:24:18.238963 140445722748800 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/visualization_utils.py:498: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, there are two\n",
            "    options available in V2.\n",
            "    - tf.py_function takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
            "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
            "    stateful argument making all functions stateful.\n",
            "    \n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/visualization_utils.py:1044: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "W0331 20:24:18.417816 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/visualization_utils.py:1044: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:484: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "W0331 20:24:18.446505 140445722748800 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py:484: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
            "\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "I0331 20:24:18.839303 140445722748800 estimator.py:1150] Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2020-03-31T20:24:18Z\n",
            "I0331 20:24:18.861011 140445722748800 evaluation.py:255] Starting evaluation at 2020-03-31T20:24:18Z\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "I0331 20:24:19.394191 140445722748800 monitored_session.py:240] Graph was finalized.\n",
            "2020-03-31 20:24:19.395315: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:24:19.395862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-31 20:24:19.395981: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-31 20:24:19.396037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-03-31 20:24:19.396090: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-03-31 20:24:19.396137: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-03-31 20:24:19.396220: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-03-31 20:24:19.396267: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-03-31 20:24:19.396325: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-31 20:24:19.396517: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:24:19.397339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:24:19.397987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-31 20:24:19.398064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-31 20:24:19.398089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-31 20:24:19.398108: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-31 20:24:19.398288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:24:19.398978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-31 20:24:19.399751: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-4878\n",
            "I0331 20:24:19.402000 140445722748800 saver.py:1284] Restoring parameters from training/model.ckpt-4878\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "I0331 20:24:20.379865 140445722748800 session_manager.py:500] Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "I0331 20:24:20.493899 140445722748800 session_manager.py:502] Done running local_init_op.\n",
            "INFO:tensorflow:Performing evaluation on 1 images.\n",
            "I0331 20:24:23.064221 140442311968512 coco_evaluation.py:205] Performing evaluation on 1 images.\n",
            "creating index...\n",
            "index created!\n",
            "INFO:tensorflow:Loading and preparing annotation results...\n",
            "I0331 20:24:23.064999 140442311968512 coco_tools.py:115] Loading and preparing annotation results...\n",
            "INFO:tensorflow:DONE (t=0.00s)\n",
            "I0331 20:24:23.065424 140442311968512 coco_tools.py:137] DONE (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "2020-03-31 20:24:23.071972: W tensorflow/core/framework/op_kernel.cc:1639] Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\n",
            "    num = operator.index(num)\n",
            "\n",
            "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\n",
            "    self._metrics = self.evaluate()\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\n",
            "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\n",
            "    iouType=iou_type)\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
            "    self.params = Params(iouType=iouType) # parameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
            "    self.setDetParams()\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
            "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
            "\n",
            "  File \"<__array_function__ internals>\", line 6, in linspace\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\n",
            "    .format(type(num)))\n",
            "\n",
            "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n",
            "\t [[{{node IteratorGetNext}}]]\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/evaluation.py\", line 272, in _evaluate_once\n",
            "    session.run(eval_ops, feed_dict)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1360, in run\n",
            "    raise six.reraise(*original_exc_info)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 693, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1418, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1176, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.OutOfRangeError: End of sequence\n",
            "\t [[node IteratorGetNext (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "\n",
            "Original stack trace for 'IteratorGetNext':\n",
            "  File \"object_detection/model_main.py\", line 109, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"object_detection/model_main.py\", line 105, in main\n",
            "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
            "    return executor.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
            "    return self.run_local()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
            "    saving_listeners=saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n",
            "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n",
            "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n",
            "    saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1494, in _train_with_estimator_spec\n",
            "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1426, in run\n",
            "    run_metadata=run_metadata))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 594, in after_run\n",
            "    if self._save(run_context.session, global_step):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 619, in _save\n",
            "    if l.after_save(session, step):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\n",
            "    self._evaluate(global_step_value)  # updates self.eval_result\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\n",
            "    self._evaluator.evaluate_and_export())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\n",
            "    hooks=self._eval_spec.hooks)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 480, in evaluate\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 522, in _actual_eval\n",
            "    return _evaluate()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 504, in _evaluate\n",
            "    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _evaluate_build_graph\n",
            "    self._call_model_fn_eval(input_fn, self.config))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1544, in _call_model_fn_eval\n",
            "    input_fn, ModeKeys.EVAL)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1025, in _get_features_and_labels_from_input_fn\n",
            "    self._call_input_fn(input_fn, mode))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/util.py\", line 65, in parse_input_fn_result\n",
            "    result = iterator.get_next()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/data/ops/iterator_ops.py\", line 426, in get_next\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_dataset_ops.py\", line 2518, in iterator_get_next\n",
            "    output_shapes=output_shapes, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n",
            "    attrs, op_def, compute_device)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1365, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1350, in _run_fn\n",
            "    target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1443, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\n",
            "  (0) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\n",
            "    num = operator.index(num)\n",
            "\n",
            "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\n",
            "    self._metrics = self.evaluate()\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\n",
            "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\n",
            "    iouType=iou_type)\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
            "    self.params = Params(iouType=iouType) # parameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
            "    self.setDetParams()\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
            "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
            "\n",
            "  File \"<__array_function__ internals>\", line 6, in linspace\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\n",
            "    .format(type(num)))\n",
            "\n",
            "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "\n",
            "\n",
            "\t [[{{node PyFunc_3}}]]\n",
            "\t [[cond/Detections_Left_Groundtruth_Right/0/_2331]]\n",
            "  (1) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\n",
            "    num = operator.index(num)\n",
            "\n",
            "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\n",
            "    self._metrics = self.evaluate()\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\n",
            "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\n",
            "    iouType=iou_type)\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
            "    self.params = Params(iouType=iouType) # parameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
            "    self.setDetParams()\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
            "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
            "\n",
            "  File \"<__array_function__ internals>\", line 6, in linspace\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\n",
            "    .format(type(num)))\n",
            "\n",
            "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "\n",
            "\n",
            "\t [[{{node PyFunc_3}}]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"object_detection/model_main.py\", line 109, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"object_detection/model_main.py\", line 105, in main\n",
            "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
            "    return executor.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
            "    return self.run_local()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
            "    saving_listeners=saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n",
            "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n",
            "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n",
            "    saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1494, in _train_with_estimator_spec\n",
            "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1360, in run\n",
            "    raise six.reraise(*original_exc_info)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/six.py\", line 693, in reraise\n",
            "    raise value\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1426, in run\n",
            "    run_metadata=run_metadata))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 594, in after_run\n",
            "    if self._save(run_context.session, global_step):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 619, in _save\n",
            "    if l.after_save(session, step):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\n",
            "    self._evaluate(global_step_value)  # updates self.eval_result\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\n",
            "    self._evaluator.evaluate_and_export())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\n",
            "    hooks=self._eval_spec.hooks)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 480, in evaluate\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 522, in _actual_eval\n",
            "    return _evaluate()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 511, in _evaluate\n",
            "    output_dir=self.eval_dir(name))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1619, in _evaluate_run\n",
            "    config=self._session_config)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/evaluation.py\", line 272, in _evaluate_once\n",
            "    session.run(eval_ops, feed_dict)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 861, in __exit__\n",
            "    self._close_internal(exception_type)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 894, in _close_internal\n",
            "    h.end(self._coordinated_creator.tf_sess)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 951, in end\n",
            "    self._final_ops, feed_dict=self._final_ops_feed_dict)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 956, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1180, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1359, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\", line 1384, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.InvalidArgumentError: 2 root error(s) found.\n",
            "  (0) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\n",
            "    num = operator.index(num)\n",
            "\n",
            "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\n",
            "    self._metrics = self.evaluate()\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\n",
            "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\n",
            "    iouType=iou_type)\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
            "    self.params = Params(iouType=iouType) # parameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
            "    self.setDetParams()\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
            "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
            "\n",
            "  File \"<__array_function__ internals>\", line 6, in linspace\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\n",
            "    .format(type(num)))\n",
            "\n",
            "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "\n",
            "\n",
            "\t [[node PyFunc_3 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "\t [[cond/Detections_Left_Groundtruth_Right/0/_2331]]\n",
            "  (1) Invalid argument: TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 117, in linspace\n",
            "    num = operator.index(num)\n",
            "\n",
            "TypeError: 'numpy.float64' object cannot be interpreted as an integer\n",
            "\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 235, in __call__\n",
            "    ret = func(*args)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 384, in first_value_func\n",
            "    self._metrics = self.evaluate()\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 215, in evaluate\n",
            "    coco_wrapped_groundtruth, coco_wrapped_detections, agnostic_mode=False)\n",
            "\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_tools.py\", line 176, in __init__\n",
            "    iouType=iou_type)\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 76, in __init__\n",
            "    self.params = Params(iouType=iouType) # parameters\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 527, in __init__\n",
            "    self.setDetParams()\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/pycocotools/cocoeval.py\", line 507, in setDetParams\n",
            "    self.iouThrs = np.linspace(.5, 0.95, np.round((0.95 - .5) / .05) + 1, endpoint=True)\n",
            "\n",
            "  File \"<__array_function__ internals>\", line 6, in linspace\n",
            "\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\", line 121, in linspace\n",
            "    .format(type(num)))\n",
            "\n",
            "TypeError: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer.\n",
            "\n",
            "\n",
            "\t [[node PyFunc_3 (defined at /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py:1748) ]]\n",
            "0 successful operations.\n",
            "0 derived errors ignored.\n",
            "\n",
            "Original stack trace for 'PyFunc_3':\n",
            "  File \"object_detection/model_main.py\", line 109, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"object_detection/model_main.py\", line 105, in main\n",
            "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_specs[0])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 473, in train_and_evaluate\n",
            "    return executor.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 613, in run\n",
            "    return self.run_local()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 714, in run_local\n",
            "    saving_listeners=saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 370, in train\n",
            "    loss = self._train_model(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1161, in _train_model\n",
            "    return self._train_model_default(input_fn, hooks, saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1195, in _train_model_default\n",
            "    saving_listeners)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1494, in _train_with_estimator_spec\n",
            "    _, loss = mon_sess.run([estimator_spec.train_op, estimator_spec.loss])\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 754, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1259, in run\n",
            "    run_metadata=run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1345, in run\n",
            "    return self._sess.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/monitored_session.py\", line 1426, in run\n",
            "    run_metadata=run_metadata))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 594, in after_run\n",
            "    if self._save(run_context.session, global_step):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/basic_session_run_hooks.py\", line 619, in _save\n",
            "    if l.after_save(session, step):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 519, in after_save\n",
            "    self._evaluate(global_step_value)  # updates self.eval_result\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 539, in _evaluate\n",
            "    self._evaluator.evaluate_and_export())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/training.py\", line 920, in evaluate_and_export\n",
            "    hooks=self._eval_spec.hooks)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 480, in evaluate\n",
            "    name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 522, in _actual_eval\n",
            "    return _evaluate()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 504, in _evaluate\n",
            "    self._evaluate_build_graph(input_fn, hooks, checkpoint_path))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1511, in _evaluate_build_graph\n",
            "    self._call_model_fn_eval(input_fn, self.config))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1547, in _call_model_fn_eval\n",
            "    features, labels, ModeKeys.EVAL, config)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_estimator/python/estimator/estimator.py\", line 1149, in _call_model_fn\n",
            "    model_fn_results = self._model_fn(features=features, **kwargs)\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/model_lib.py\", line 482, in model_fn\n",
            "    eval_config, list(category_index.values()), eval_dict)\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/eval_util.py\", line 947, in get_eval_metric_ops_for_evaluators\n",
            "    eval_dict))\n",
            "  File \"/content/drive/My Drive/objectDetection/models/research/object_detection/metrics/coco_evaluation.py\", line 394, in get_estimator_eval_metric_ops\n",
            "    first_value_op = tf.py_func(first_value_func, [], tf.float32)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 324, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 513, in py_func\n",
            "    return py_func_common(func, inp, Tout, stateful, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 495, in py_func_common\n",
            "    func=func, inp=inp, Tout=Tout, stateful=stateful, eager=False, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/script_ops.py\", line 318, in _internal_py_func\n",
            "    input=inp, token=token, Tout=Tout, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/gen_script_ops.py\", line 170, in py_func\n",
            "    \"PyFunc\", input=input, token=token, Tout=Tout, name=name)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/op_def_library.py\", line 794, in _apply_op_helper\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/deprecation.py\", line 507, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3357, in create_op\n",
            "    attrs, op_def, compute_device)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 3426, in _create_op_internal\n",
            "    op_def=op_def)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/ops.py\", line 1748, in __init__\n",
            "    self._traceback = tf_stack.extract_stack()\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWzZs5We43MO",
        "colab_type": "code",
        "outputId": "20919a3b-2d0b-41a3-ae47-4be2fe4ae435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "output_directory = './fine_tunedmodel'\n",
        "\n",
        "lst = os.listdir('training')\n",
        "print(lst)\n",
        "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
        "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
        "last_model = lst[steps.argmax()].replace('.meta', '')\n",
        "\n",
        "last_model_path = os.path.join('training', last_model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['events.out.tfevents.1585512893.31bea5f2605f', 'events.out.tfevents.1585513604.af95399df642', 'events.out.tfevents.1585514489.af95399df642', 'events.out.tfevents.1585683581.3ed06592069b', 'events.out.tfevents.1585684309.3ed06592069b', 'events.out.tfevents.1585685628.3ed06592069b', 'pipeline.config', 'events.out.tfevents.1585746414.3f1bc145f75e', 'graph.pbtxt', 'model.ckpt-33158.data-00000-of-00001', 'model.ckpt-33158.index', 'model.ckpt-33158.meta', 'model.ckpt-34168.data-00000-of-00001', 'model.ckpt-34168.index', 'model.ckpt-34168.meta', 'model.ckpt-35171.data-00000-of-00001', 'model.ckpt-35171.index', 'model.ckpt-35171.meta', 'model.ckpt-36184.data-00000-of-00001', 'model.ckpt-36184.index', 'model.ckpt-36184.meta', 'model.ckpt-37198.data-00000-of-00001', 'model.ckpt-37198.index', 'model.ckpt-37198.meta', 'checkpoint']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgI5atG85s9d",
        "colab_type": "code",
        "outputId": "ada5c0cd-5361-4136-f90c-ba393e2cc15c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python /content/drive/My\\ Drive/objectDetection/models/research/object_detection/export_inference_graph.py \\\n",
        "    --input_type=image_tensor \\\n",
        "    --pipeline_config_path=/content/drive/My\\ Drive/objectDetection/models/research/object_detection/samples/configs/ssd_mobilenet_v2_coco.config \\\n",
        "    --output_directory={output_directory} \\\n",
        "    --trained_checkpoint_prefix={last_model_path}"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/export_inference_graph.py:162: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0402 03:30:47.323463 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/export_inference_graph.py:145: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "W0402 03:30:47.330385 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:402: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0402 03:30:47.330762 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:121: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "W0402 03:30:47.359065 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/core/preprocessor.py:2937: The name tf.image.resize_images is deprecated. Please use tf.image.resize instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:589: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0402 03:30:47.383644 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:589: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0402 03:30:47.383825 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/meta_architectures/ssd_meta_arch.py:597: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0402 03:30:47.386432 139803824547712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1057: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "W0402 03:30:49.304847 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/core/anchor_generator.py:171: The name tf.assert_equal is deprecated. Please use tf.compat.v1.assert_equal instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "W0402 03:30:49.314369 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/predictors/convolutional_box_predictor.py:150: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0402 03:30:49.314537 139803824547712 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0402 03:30:49.350601 139803824547712 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0402 03:30:49.385629 139803824547712 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0402 03:30:49.420080 139803824547712 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0402 03:30:49.453821 139803824547712 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
            "I0402 03:30:49.488497 139803824547712 convolutional_box_predictor.py:151] depth of additional conv before box predictor: 0\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0402 03:30:49.818182 139803824547712 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/core/post_processing.py:581: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0402 03:30:50.105353 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:278: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "W0402 03:30:50.105619 139803824547712 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:383: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0402 03:30:50.108365 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:415: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "W0402 03:30:50.108529 139803824547712 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:539: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
            "Instructions for updating:\n",
            "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "W0402 03:30:50.109398 139803824547712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
            "133 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              0\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   name\n",
            "-account_type_regexes       _trainable_variables\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     params\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "param: Number of parameters (in the Variable).\n",
            "\n",
            "Profile:\n",
            "node name | # parameters\n",
            "_TFProfRoot (--/4.57m params)\n",
            "  BoxPredictor_0 (--/10.39k params)\n",
            "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
            "    BoxPredictor_0/ClassPredictor (--/3.46k params)\n",
            "      BoxPredictor_0/ClassPredictor/biases (6, 6/6 params)\n",
            "      BoxPredictor_0/ClassPredictor/weights (1x1x576x6, 3.46k/3.46k params)\n",
            "  BoxPredictor_1 (--/46.12k params)\n",
            "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
            "    BoxPredictor_1/ClassPredictor (--/15.37k params)\n",
            "      BoxPredictor_1/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x12, 15.36k/15.36k params)\n",
            "  BoxPredictor_2 (--/18.47k params)\n",
            "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
            "    BoxPredictor_2/ClassPredictor (--/6.16k params)\n",
            "      BoxPredictor_2/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_2/ClassPredictor/weights (1x1x512x12, 6.14k/6.14k params)\n",
            "  BoxPredictor_3 (--/9.25k params)\n",
            "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_3/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_3/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_3/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "  BoxPredictor_4 (--/9.25k params)\n",
            "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
            "    BoxPredictor_4/ClassPredictor (--/3.08k params)\n",
            "      BoxPredictor_4/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_4/ClassPredictor/weights (1x1x256x12, 3.07k/3.07k params)\n",
            "  BoxPredictor_5 (--/4.64k params)\n",
            "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
            "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
            "    BoxPredictor_5/ClassPredictor (--/1.55k params)\n",
            "      BoxPredictor_5/ClassPredictor/biases (12, 12/12 params)\n",
            "      BoxPredictor_5/ClassPredictor/weights (1x1x128x12, 1.54k/1.54k params)\n",
            "  FeatureExtractor (--/4.48m params)\n",
            "    FeatureExtractor/MobilenetV2 (--/4.48m params)\n",
            "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
            "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
            "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
            "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/1.18m params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/294.91k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/294.91k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
            "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/73.73k params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
            "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)\n",
            "\n",
            "======================End of Report==========================\n",
            "133 ops no flops stats due to incomplete shapes.\n",
            "Parsing Inputs...\n",
            "Incomplete shape.\n",
            "\n",
            "=========================Options=============================\n",
            "-max_depth                  10000\n",
            "-min_bytes                  0\n",
            "-min_peak_bytes             0\n",
            "-min_residual_bytes         0\n",
            "-min_output_bytes           0\n",
            "-min_micros                 0\n",
            "-min_accelerator_micros     0\n",
            "-min_cpu_micros             0\n",
            "-min_params                 0\n",
            "-min_float_ops              1\n",
            "-min_occurrence             0\n",
            "-step                       -1\n",
            "-order_by                   float_ops\n",
            "-account_type_regexes       .*\n",
            "-start_name_regexes         .*\n",
            "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
            "-show_name_regexes          .*\n",
            "-hide_name_regexes          \n",
            "-account_displayed_op_only  true\n",
            "-select                     float_ops\n",
            "-output                     stdout:\n",
            "\n",
            "==================Model Analysis Report======================\n",
            "Incomplete shape.\n",
            "\n",
            "Doc:\n",
            "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
            "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
            "\n",
            "Profile:\n",
            "node name | # float_ops\n",
            "_TFProfRoot (--/13.71k flops)\n",
            "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
            "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
            "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
            "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
            "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
            "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
            "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
            "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
            "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
            "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
            "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
            "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
            "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
            "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
            "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
            "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
            "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
            "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
            "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
            "  Preprocessor/map/while/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
            "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
            "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
            "\n",
            "======================End of Report==========================\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "W0402 03:30:50.974575 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:432: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0402 03:30:51.626125 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:342: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2020-04-02 03:30:51.627349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-04-02 03:30:51.640156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.640742: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-04-02 03:30:51.641020: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-04-02 03:30:51.642151: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-04-02 03:30:51.643305: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-04-02 03:30:51.643676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-04-02 03:30:51.645372: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-04-02 03:30:51.646574: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-04-02 03:30:51.650670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-04-02 03:30:51.650779: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.651339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.651841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-04-02 03:30:51.652124: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-04-02 03:30:51.656697: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000155000 Hz\n",
            "2020-04-02 03:30:51.656879: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3260d80 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-04-02 03:30:51.656906: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-04-02 03:30:51.740979: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.741634: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x3260f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-04-02 03:30:51.741663: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
            "2020-04-02 03:30:51.741822: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.742341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-04-02 03:30:51.742396: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-04-02 03:30:51.742411: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-04-02 03:30:51.742423: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-04-02 03:30:51.742435: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-04-02 03:30:51.742448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-04-02 03:30:51.742460: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-04-02 03:30:51.742473: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-04-02 03:30:51.742525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.743078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.743594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-04-02 03:30:51.743670: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-04-02 03:30:51.744856: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-04-02 03:30:51.744884: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-04-02 03:30:51.744893: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-04-02 03:30:51.744991: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.745523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:51.746039: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-04-02 03:30:51.746076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-37198\n",
            "I0402 03:30:51.748632 139803824547712 saver.py:1284] Restoring parameters from training/model.ckpt-37198\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0402 03:30:53.509136 139803824547712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "2020-04-02 03:30:53.969290: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:53.969882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-04-02 03:30:53.969972: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-04-02 03:30:53.969989: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-04-02 03:30:53.970005: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-04-02 03:30:53.970018: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-04-02 03:30:53.970030: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-04-02 03:30:53.970044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-04-02 03:30:53.970059: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-04-02 03:30:53.970134: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:53.970634: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:53.971110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-04-02 03:30:53.971141: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-04-02 03:30:53.971151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-04-02 03:30:53.971159: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-04-02 03:30:53.971226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:53.971731: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:53.972207: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "INFO:tensorflow:Restoring parameters from training/model.ckpt-37198\n",
            "I0402 03:30:53.974241 139803824547712 saver.py:1284] Restoring parameters from training/model.ckpt-37198\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0402 03:30:54.537271 139803824547712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "W0402 03:30:54.537499 139803824547712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
            "INFO:tensorflow:Froze 324 variables.\n",
            "I0402 03:30:54.850238 139803824547712 graph_util_impl.py:334] Froze 324 variables.\n",
            "INFO:tensorflow:Converted 324 variables to const ops.\n",
            "I0402 03:30:54.922764 139803824547712 graph_util_impl.py:394] Converted 324 variables to const ops.\n",
            "2020-04-02 03:30:55.068067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:55.068649: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla P100-PCIE-16GB major: 6 minor: 0 memoryClockRate(GHz): 1.3285\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-04-02 03:30:55.068729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-04-02 03:30:55.068745: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-04-02 03:30:55.068758: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-04-02 03:30:55.068771: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-04-02 03:30:55.068787: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-04-02 03:30:55.068799: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-04-02 03:30:55.068812: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-04-02 03:30:55.068876: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:55.069381: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:55.069854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-04-02 03:30:55.069888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-04-02 03:30:55.069897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-04-02 03:30:55.069905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-04-02 03:30:55.069971: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:55.070494: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-04-02 03:30:55.070974: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 15216 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "W0402 03:30:55.410301 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:306: The name tf.saved_model.builder.SavedModelBuilder is deprecated. Please use tf.compat.v1.saved_model.builder.SavedModelBuilder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0402 03:30:55.413480 139803824547712 deprecation.py:323] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:309: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "W0402 03:30:55.414149 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:315: The name tf.saved_model.signature_def_utils.build_signature_def is deprecated. Please use tf.compat.v1.saved_model.signature_def_utils.build_signature_def instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "W0402 03:30:55.414399 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:318: The name tf.saved_model.signature_constants.PREDICT_METHOD_NAME is deprecated. Please use tf.saved_model.PREDICT_METHOD_NAME instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "W0402 03:30:55.414746 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:323: The name tf.saved_model.tag_constants.SERVING is deprecated. Please use tf.saved_model.SERVING instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "W0402 03:30:55.414969 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/exporter.py:325: The name tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY is deprecated. Please use tf.saved_model.DEFAULT_SERVING_SIGNATURE_DEF_KEY instead.\n",
            "\n",
            "INFO:tensorflow:No assets to save.\n",
            "I0402 03:30:55.415343 139803824547712 builder_impl.py:640] No assets to save.\n",
            "INFO:tensorflow:No assets to write.\n",
            "I0402 03:30:55.415466 139803824547712 builder_impl.py:460] No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: ./fine_tunedmodel/saved_model/saved_model.pb\n",
            "I0402 03:30:55.839859 139803824547712 builder_impl.py:425] SavedModel written to: ./fine_tunedmodel/saved_model/saved_model.pb\n",
            "WARNING:tensorflow:From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0402 03:30:55.864357 139803824547712 module_wrapper.py:139] From /content/drive/My Drive/objectDetection/models/research/object_detection/utils/config_util.py:188: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "INFO:tensorflow:Writing pipeline config file to ./fine_tunedmodel/pipeline.config\n",
            "I0402 03:30:55.864573 139803824547712 config_util.py:190] Writing pipeline config file to ./fine_tunedmodel/pipeline.config\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDZgBDGh7O_1",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0F8Oin8A6cAD",
        "colab_type": "code",
        "outputId": "ec87951e-7998-484c-cb45-e34876ac8341",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "#downloads the frozen model that is needed for inference\n",
        "files.download(output_directory + '/frozen_inference_graph.pb')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0324dcb98801>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_directory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/frozen_inference_graph.pb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: NetworkError when attempting to fetch resource.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 1.15.0. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o2ER_Dr6hTz",
        "colab_type": "code",
        "outputId": "1848e64b-04b3-48c8-f253-d43be581f470",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        }
      },
      "source": [
        "data_base_url='/content/drive/My Drive/objectDetection'\n",
        "#download the label map\n",
        "files.download(data_base_url + '/label_map.pbtxt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-32e53b803562>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_base_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/objectDetection'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#download the label map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_base_url\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/label_map.pbtxt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: NetworkError when attempting to fetch resource."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1trAqxa-NjgB",
        "colab_type": "code",
        "outputId": "b4c54056-4934-4639-e6fc-314c7aa5689b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        }
      },
      "source": [
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# This is needed since the notebook is stored in the object_detection folder.\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "# Import utilites\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as vis_util\n",
        "\n",
        "\n",
        "# Path to frozen detection graph .pb file, which contains the model that is used\n",
        "# for object detection.\n",
        "PATH_TO_CKPT = '/content/drive/My Drive/objectDetection/models/research/fine_tunedmodel/frozen_inference_graph.pb'\n",
        "\n",
        "# Path to label map file\n",
        "PATH_TO_LABELS = '/content/drive/My Drive/objectDetection/label_map.pbtxt'\n",
        "\n",
        "# Path to image\n",
        "PATH_TO_IMAGE = '/content/drive/My Drive/objectDetection/train/20.jpg'\n",
        "\n",
        "# Number of classes the object detector can identify\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# Load the label map.\n",
        "# Label maps map indices to category names, so that when our convolution\n",
        "# network predicts `5`, we know that this corresponds to `king`.\n",
        "# Here we use internal utility functions, but anything that returns a\n",
        "# dictionary mapping integers to appropriate string labels would be fine\n",
        "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
        "categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\n",
        "category_index = label_map_util.create_category_index(categories)\n",
        "\n",
        "# Load the Tensorflow model into memory.\n",
        "detection_graph = tf.Graph()\n",
        "with detection_graph.as_default():\n",
        "    od_graph_def = tf.GraphDef()\n",
        "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
        "        serialized_graph = fid.read()\n",
        "        od_graph_def.ParseFromString(serialized_graph)\n",
        "        tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "    sess = tf.Session(graph=detection_graph)\n",
        "\n",
        "# Define input and output tensors (i.e. data) for the object detection classifier\n",
        "\n",
        "# Input tensor is the image\n",
        "image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "\n",
        "# Output tensors are the detection boxes, scores, and classes\n",
        "# Each box represents a part of the image where a particular object was detected\n",
        "detection_boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "\n",
        "# Each score represents level of confidence for each of the objects.\n",
        "# The score is shown on the result image, together with the class label.\n",
        "detection_scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "detection_classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "\n",
        "# Number of objects detected\n",
        "num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "# Load image using OpenCV and\n",
        "# expand image dimensions to have shape: [1, None, None, 3]\n",
        "# i.e. a single-column array, where each item in the column has the pixel RGB value\n",
        "image = cv2.imread(PATH_TO_IMAGE)\n",
        "image_expanded = np.expand_dims(image, axis=0)\n",
        "\n",
        "# Perform the actual detection by running the model with the image as input\n",
        "(boxes, scores, classes, num) = sess.run(\n",
        "    [detection_boxes, detection_scores, detection_classes, num_detections],\n",
        "    feed_dict={image_tensor: image_expanded})\n",
        "\n",
        "# Draw the results of the detection (aka 'visulaize the results')\n",
        "\n",
        "vis_util.visualize_boxes_and_labels_on_image_array(\n",
        "    image,\n",
        "    np.squeeze(boxes),\n",
        "    np.squeeze(classes).astype(np.int32),\n",
        "    np.squeeze(scores),\n",
        "    category_index,\n",
        "    use_normalized_coordinates=True,\n",
        "    line_thickness=8,\n",
        "    min_score_thresh=0.80)\n",
        "\n",
        "# All the results have been drawn on image. Now display the image.\n",
        "cv2_imshow(image)\n",
        "\n",
        "# Press any key to close the image\n",
        "cv2.waitKey(0)\n",
        "\n",
        "# Clean up\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOEAAADhCAIAAACx0UUtAADs/klEQVR4nOz9abhsx3EYCEZEZp6l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\n0n8EzPCAgIIbwwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=225x225 at 0x7F775C2DA898>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}